import re
import base64
from pathlib import Path
from DrissionPage import Chromium
from gmaps_crawler.pipeline.extractors.web_extractors.utils import extract_base64_links


def extract_emails_phones_socials(page: Chromium, websites: list) -> dict:
    """
    ä»å¤šä¸ªç½‘ç«™æå–é‚®ç®±ã€ç”µè¯ã€ç¤¾äº¤åª’ä½“é“¾æ¥ï¼ˆå«Base64è§£æï¼‰ã€‚
    è‡ªåŠ¨è¿‡æ»¤ URL ä¸­çš„ '@'ï¼ˆå¦‚ Sentry DSNï¼‰ï¼Œé¿å…è¯¯è¯†åˆ«ã€‚
    è¿”å›:
    {
        "emails": [ {"email": str, "source_url": str}, ... ],
        "phones": [str],
        "socials": {å¹³å°: æœ€çŸ­URLæˆ–ç©ºå­—ç¬¦ä¸²},
        "per_site": {
            ç½‘ç«™URL: {
                "emails": [str],
                "phones": [str],
                "socials": {å¹³å°: æœ€çŸ­URLæˆ–ç©ºå­—ç¬¦ä¸²}
            }
        }
    }
    """
    if not websites:
        return {"emails": [], "phones": [], "socials": {}, "per_site": {}}

    # ç¡®ä¿URLæ ¼å¼
    websites = [
        f"http://{url}" if not url.startswith(("http://", "https://")) else url
        for url in websites
    ]

    tab = page.new_tab()

    # æ”¹è¿›ç‰ˆæ­£åˆ™ï¼ˆæ’é™¤URLä¸­çš„@ï¼‰
    email_pattern = r'(?<![\/:@])\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b(?![\/:])'
    phone_pattern = r'\+\d[\d\s\-]{6,15}\d'
    url_pattern = r'https?://[^\s"\'<>]+'

    # å¸¸è§ç¤¾äº¤å¹³å°åŸŸå
    social_domains = {
        "facebook": ["facebook.com"],
        "instagram": ["instagram.com"],
        "twitter": ["twitter.com", "x.com"],
        "linkedin": ["linkedin.com"],
        "youtube": ["youtube.com", "youtu.be"],
        "tiktok": ["tiktok.com"],
        "whatsapp": ["whatsapp.com"],
        "telegram": ["t.me", "telegram.me"],
        "yelp": ["yelp.com", "yelp.fr", "yelp.ie", "yelp.co.uk"],
    }

    # å…¨å±€å­˜å‚¨
    emails_with_source = {}
    all_phones = set()
    found_socials = {k: set() for k in social_domains}
    results_per_site = {}

    for url in websites:
        site_emails = set()
        site_phones = set()
        site_socials = {k: set() for k in social_domains}

        try:
            tab.get(url, timeout=15)
            html_content = tab.html

            # === é‚®ç®± ===
            for e in re.findall(email_pattern, html_content):
                e = e.lower()
                # æ’é™¤å¸¸è§éé‚®ç®±ç”¨é€”çš„åŸŸï¼ˆSentryã€AWSã€Cloudflareç­‰ï¼‰
                if any(x in e for x in ['sentry.io', 'amazonaws.com', 'cloudflare.com']):
                    continue
                # é‚®ç®±ä¸åº”å‡ºç°åœ¨URLå†…éƒ¨
                if re.search(r'https?://[^"\'\s]*' + re.escape(e), html_content):
                    continue
                # æ’é™¤å›¾ç‰‡æ–‡ä»¶ä¼ªè£…çš„é‚®ç®±å
                if re.search(r'\.(jpg|jpeg|png|gif|svg|webp)$', e, re.IGNORECASE):
                    continue

                emails_with_source.setdefault(e, url)
                site_emails.add(e)

            # === ç”µè¯ ===
            for p in re.findall(phone_pattern, html_content):
                clean_p = re.sub(r'\s+', '', p)
                all_phones.add(clean_p)
                site_phones.add(clean_p)

            # === æ™®é€šç¤¾äº¤é“¾æ¥ ===
            for link in re.findall(url_pattern, html_content):
                link_lower = link.lower()
                for platform, domains in social_domains.items():
                    if any(d in link_lower for d in domains):
                        found_socials[platform].add(link)
                        site_socials[platform].add(link)

            # === Base64éšè—é“¾æ¥ ===
            decoded_links = extract_base64_links(html_content)
            for d_link in decoded_links:
                for platform, domains in social_domains.items():
                    if any(d in d_link.lower() for d in domains):
                        found_socials[platform].add(d_link)
                        site_socials[platform].add(d_link)

        except Exception as e:
            print(f"âš ï¸ è®¿é—® {url} æ—¶å‡ºé”™: {e}")

        # æ¯ä¸ªç½‘ç«™å•ç‹¬ä¿å­˜
        results_per_site[url] = {
            "emails": sorted(site_emails),
            "phones": sorted(site_phones),
            "socials": {p: (min(v, key=len) if v else "") for p, v in site_socials.items()},
        }

    tab.close()

    # æ±‡æ€»ç»“æœ
    socials_summary = {p: (min(v, key=len) if v else "") for p, v in found_socials.items()}
    email_list = [{"email": e, "source_url": src} for e, src in emails_with_source.items()]

    return {
        "emails": email_list,
        "phones": sorted(all_phones),
        "socials": socials_summary,
        "per_site": results_per_site,
    }



# =======================
# æµ‹è¯•éƒ¨åˆ†
# =======================
if __name__ == "__main__":
    test_websites = [
        "https://cafecherie.fr",
        "https://www.yelp.ie/biz/cafe-cherie-boulogne",
        "https://www.facebook.com/Emilieandthecoolkids/",
        "https://www.privateaser.com/lieu/49030-cafe-cherie-brasserie-bar-a-cocktail",
    ]
    cp = Chromium(12312)
    result = extract_emails_phones_socials(cp, test_websites)
    print(result)

    print("\nğŸ“§ å…¨éƒ¨é‚®ç®±:")
    for e in result["emails"]:
        print(f"  - {e['email']} ({e['source_url']})")

    print("\nğŸ“ ç”µè¯:")
    for p in result["phones"]:
        print(f"  - {p}")

    print("\nğŸ”— ç¤¾äº¤åª’ä½“æ±‡æ€»:")
    for platform, link in result["socials"].items():
        if link:
            print(f"  {platform}: {link}")

    print("\nğŸŒ å„ç½‘ç«™è¯¦ç»†ç»“æœ:")
    for site, data in result["per_site"].items():
        print(f"\n{site}:")
        print(f"  Emails: {data['emails']}")
        print(f"  Phones: {data['phones']}")
        print(f"  Socials: {data['socials']}")
